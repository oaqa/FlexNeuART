{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation/downloading/processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we create the root collection directory and point environment variable `COLLECT_ROOT` to this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/leo/flexneuart_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: COLLECT_ROOT=/home/leo/flexneuart_collections\n"
     ]
    }
   ],
   "source": [
    "%env COLLECT_ROOT=/home/leo/flexneuart_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leo/flexneuart_collections\r\n"
     ]
    }
   ],
   "source": [
    "!bash -c \"echo $COLLECT_ROOT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works with a sub-sample of the natural question collection (__Wikipedia DPR__) prepared by [Karpukhin et al.](https://github.com/facebookresearch/DPR). This subset includes all the questions from __Wikipedia DPR__, but only a sample  of passages (about one million). \n",
    "\n",
    "The generation of this subset is briefly described below, but for your convenience we provide an archive with already processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the directory, downloaded and unpack data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leo/flexneuart_collections\n"
     ]
    }
   ],
   "source": [
    "cd /home/leo/flexneuart_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-16 15:02:07--  http://boytsov.info/datasets/wikipedia_dpr_nq_sample_conf_2021-09-15.tar.bz2\n",
      "Resolving boytsov.info (boytsov.info)... 69.60.127.165\n",
      "Connecting to boytsov.info (boytsov.info)|69.60.127.165|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2657 (2.6K) [application/x-bzip2]\n",
      "Saving to: ‘wikipedia_dpr_nq_sample_conf_2021-09-15.tar.bz2’\n",
      "\n",
      "wikipedia_dpr_nq_sa 100%[===================>]   2.59K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-09-16 15:02:07 (113 MB/s) - ‘wikipedia_dpr_nq_sample_conf_2021-09-15.tar.bz2’ saved [2657/2657]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget boytsov.info/datasets/wikipedia_dpr_nq_sample_conf_2021-09-15.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-16 19:13:14--  http://boytsov.info/datasets/wikipedia_dpr_nq_sample_models_2021-09-15.tar.bz2\n",
      "Resolving boytsov.info (boytsov.info)... 69.60.127.165\n",
      "Connecting to boytsov.info (boytsov.info)|69.60.127.165|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 414972906 (396M) [application/x-bzip2]\n",
      "Saving to: ‘wikipedia_dpr_nq_sample_models_2021-09-15.tar.bz2’\n",
      "\n",
      "wikipedia_dpr_nq_sa 100%[===================>] 395.75M  2.61MB/s    in 2m 32s  \n",
      "\n",
      "2021-09-16 19:15:47 (2.60 MB/s) - ‘wikipedia_dpr_nq_sample_models_2021-09-15.tar.bz2’ saved [414972906/414972906]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget boytsov.info/datasets/wikipedia_dpr_nq_sample_models_2021-09-15.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-15 23:57:25--  http://boytsov.info/datasets/wikipedia_dpr_nq_sample_data_2021-09-15.tar.bz2\n",
      "Resolving boytsov.info (boytsov.info)... 69.60.127.165\n",
      "Connecting to boytsov.info (boytsov.info)|69.60.127.165|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2722927168 (2.5G) [application/x-bzip2]\n",
      "Saving to: ‘wikipedia_dpr_nq_sample_data_2021-09-15.tar.bz2’\n",
      "\n",
      "wikipedia_dpr_nq_sa 100%[===================>]   2.54G  1.44MB/s    in 43m 30s \n",
      "\n",
      "2021-09-16 00:40:55 (1019 KB/s) - ‘wikipedia_dpr_nq_sample_data_2021-09-15.tar.bz2’ saved [2722927168/2722927168]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget boytsov.info/datasets/wikipedia_dpr_nq_sample_data_2021-09-15.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-16 15:33:41--  http://boytsov.info/datasets/wikipedia_dpr_nq_sample_bitext_2021-09-15.tar.bz2\n",
      "Resolving boytsov.info (boytsov.info)... 69.60.127.165\n",
      "Connecting to boytsov.info (boytsov.info)|69.60.127.165|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 43284754 (41M) [application/x-bzip2]\n",
      "Saving to: ‘wikipedia_dpr_nq_sample_bitext_2021-09-15.tar.bz2’\n",
      "\n",
      "wikipedia_dpr_nq_sa 100%[===================>]  41.28M  1.02MB/s    in 37s     \n",
      "\n",
      "2021-09-16 15:34:18 (1.12 MB/s) - ‘wikipedia_dpr_nq_sample_bitext_2021-09-15.tar.bz2’ saved [43284754/43284754]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget boytsov.info/datasets/wikipedia_dpr_nq_sample_bitext_2021-09-15.tar.bz2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-16 19:53:30--  http://boytsov.info/datasets/wikipedia_dpr_nq_sample_embed_2021-09-15.tar.bz2\n",
      "Resolving boytsov.info (boytsov.info)... 69.60.127.165\n",
      "Connecting to boytsov.info (boytsov.info)|69.60.127.165|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 55835230 (53M) [application/x-bzip2]\n",
      "Saving to: ‘wikipedia_dpr_nq_sample_embed_2021-09-15.tar.bz2.1’\n",
      "\n",
      "wikipedia_dpr_nq_sa 100%[===================>]  53.25M  2.24MB/s    in 24s     \n",
      "\n",
      "2021-09-16 19:53:54 (2.23 MB/s) - ‘wikipedia_dpr_nq_sample_embed_2021-09-15.tar.bz2.1’ saved [55835230/55835230]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget boytsov.info/datasets/wikipedia_dpr_nq_sample_embed_2021-09-15.tar.bz2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia_dpr_nq_sample/\n",
      "wikipedia_dpr_nq_sample/input_data/\n",
      "wikipedia_dpr_nq_sample/input_data/train_fusion/\n",
      "wikipedia_dpr_nq_sample/input_data/train_fusion/QuestionFields.jsonl\n",
      "wikipedia_dpr_nq_sample/input_data/train_fusion/qrels.txt\n",
      "wikipedia_dpr_nq_sample/input_data/train_fusion/QuestionFields.bin\n",
      "wikipedia_dpr_nq_sample/input_data/dev/\n",
      "wikipedia_dpr_nq_sample/input_data/dev/QuestionFields.jsonl\n",
      "wikipedia_dpr_nq_sample/input_data/dev/qrels.txt\n",
      "wikipedia_dpr_nq_sample/input_data/dev/QuestionFields.bin\n",
      "wikipedia_dpr_nq_sample/input_data/bitext/\n",
      "wikipedia_dpr_nq_sample/input_data/bitext/QuestionFields.jsonl\n",
      "wikipedia_dpr_nq_sample/input_data/bitext/qrels.txt\n",
      "wikipedia_dpr_nq_sample/input_data/pass_sample/\n",
      "wikipedia_dpr_nq_sample/input_data/pass_sample/AnswerFields.bin\n",
      "wikipedia_dpr_nq_sample/input_data/pass_sample/AnswerFields.jsonl.gz\n",
      "wikipedia_dpr_nq_sample/input_data/dev_official/\n",
      "wikipedia_dpr_nq_sample/input_data/dev_official/QuestionFields.jsonl\n",
      "wikipedia_dpr_nq_sample/input_data/dev_official/qrels.txt\n",
      "wikipedia_dpr_nq_sample/input_data/dev_official/QuestionFields.bin\n"
     ]
    }
   ],
   "source": [
    "!tar jxvf wikipedia_dpr_nq_sample_data_2021-09-15.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia_dpr_nq_sample/\r\n",
      "wikipedia_dpr_nq_sample/model_conf/\r\n",
      "wikipedia_dpr_nq_sample/model_conf/vanilla_bert.json\r\n",
      "wikipedia_dpr_nq_sample/model_conf/vanilla_bert_with_scores.json\r\n",
      "wikipedia_dpr_nq_sample/derived_data/\r\n",
      "wikipedia_dpr_nq_sample/derived_data/ir_models/\r\n",
      "wikipedia_dpr_nq_sample/derived_data/ir_models/vanilla_bert/\r\n",
      "wikipedia_dpr_nq_sample/derived_data/ir_models/vanilla_bert/todays_experiment/\r\n",
      "wikipedia_dpr_nq_sample/derived_data/ir_models/vanilla_bert/todays_experiment/0/\r\n",
      "wikipedia_dpr_nq_sample/derived_data/ir_models/vanilla_bert/todays_experiment/0/vanilla_bert.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/extractors/\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/extractors/avgembed.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/extractors/cedr8080.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25_ance_exported_sparse.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25=text+model1=text_bert_tok+lambda=0.3+probSelfTran=0.35.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25_ance.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25_cedr8080.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25_avgembed.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/extractors/bm25.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/extractors/ance.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/avgembed.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/cedr8080.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/bm25_ance_exported_sparse.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib_ance.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib_bm25_ance.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/bm25_ance.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/lucene.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/bm25_cedr8080.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib/\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_ance_interleaved/\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_ance_interleaved/cand_prov.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_avgembed/\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_avgembed/cand_prov.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib/ance/\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib/ance/cand_prov.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_ance/\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib/bm25_ance/cand_prov.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/bm25_avgembed.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib_bm25_ance_interleaved.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/bm25_model1.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/bm25.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/nmslib_bm25_avgembed.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/models/\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/models/bm25_model1.model\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/models/one_feat.model\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/models/bm25_ance.model\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/bm25_test_interm.json\r\n",
      "wikipedia_dpr_nq_sample/exper_desc.best/ance.json\r\n"
     ]
    }
   ],
   "source": [
    "!tar jxvf wikipedia_dpr_nq_sample_conf_2021-09-15.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia_dpr_nq_sample/derived_data/bitext/\n",
      "wikipedia_dpr_nq_sample/derived_data/bitext/answer_text_unlemm\n",
      "wikipedia_dpr_nq_sample/derived_data/bitext/question_text_bert_tok\n",
      "wikipedia_dpr_nq_sample/derived_data/bitext/answer_text_bert_tok\n",
      "wikipedia_dpr_nq_sample/derived_data/bitext/question_text_unlemm\n"
     ]
    }
   ],
   "source": [
    "!tar jxvf wikipedia_dpr_nq_sample_bitext_2021-09-15.tar.bz2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia_dpr_nq_sample/derived_data/ir_models/vanilla_bert/model.best\r\n"
     ]
    }
   ],
   "source": [
    "!tar jxvf wikipedia_dpr_nq_sample_models_2021-09-15.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia_dpr_nq_sample/derived_data/embeddings/\r\n",
      "wikipedia_dpr_nq_sample/derived_data/embeddings/glove/\r\n",
      "wikipedia_dpr_nq_sample/derived_data/embeddings/glove/glove.6B.50d.txt.bz2\r\n"
     ]
    }
   ],
   "source": [
    "!tar jxvf wikipedia_dpr_nq_sample_embed_2021-09-15.tar.bz2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For all the following experiments we use scripts installed via `flexneuart_install_extra.sh`. They must be called from their respective installation directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leo/flexneuart_scripts\n"
     ]
    }
   ],
   "source": [
    "cd /home/leo/flexneuart_scripts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carry out a basic sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: /home/leo/flexneuart_collections\n",
      "Checking input sub-directory: bitext\n",
      "Checking input sub-directory: dev\n",
      "Checking input sub-directory: dev_official\n",
      "Checking input sub-directory: pass_sample\n",
      "Found indexable data file: pass_sample/AnswerFields.jsonl.gz\n",
      "Checking input sub-directory: train_fusion\n",
      "Found query file: bitext/QuestionFields.jsonl\n",
      "Found query file: dev/QuestionFields.jsonl\n",
      "Found query file: dev_official/QuestionFields.jsonl\n",
      "Found query file: train_fusion/QuestionFields.jsonl\n",
      "getIndexQueryDataDirs return value:  pass_sample AnswerFields.jsonl.gz bitext,dev,dev_official,train_fusion\n",
      "Using data file: AnswerFields.jsonl.gz\n",
      "Index dirs: pass_sample\n",
      "Query dirs: bitext dev dev_official train_fusion\n",
      "Queries/questions:\n",
      "bitext 53880\n",
      "dev 2500\n",
      "dev_official 6515\n",
      "train_fusion 2500\n",
      "Documents/passages/answers:\n",
      "pass_sample 774392\n"
     ]
    }
   ],
   "source": [
    "!report/get_basic_collect_stat.sh wikipedia_dpr_nq_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing in more details : This is for information purposes only because the downloaded data is already pre-processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The download and conversion script can be found in the directory `data_convert/wikipedia_dpr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting passages and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!data_convert/wikipedia_dpr/download_dpr_passages.sh $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!data_convert/wikipedia_dpr/download_dpr_queries.sh nq $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly split the training set into the new training and development sets. This script also converts the data into FlexNeuART format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!data_convert/wikipedia_dpr/split_and_convert_dpr_queries.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    collections/wikipedia_dpr_nq_sample/input_raw/ \\\n",
    "    nq \\\n",
    "    -partition_sizes ,5000,2500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The split & convert script produces outputs of two types:\n",
    "1. The set of questions in JSONL format. These questions are divided into several subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bitext` subset and the `train_fusion` subsets are supposed to be used to train models. The difference is that `train_fusion` is a smaller subset that can be used to create fusion models. The `bitext` part can be used to train, e.g., neural models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the queries from the `bitext` set, the conversion script creates parallel data (bitext) where questions are aligned with respective answer-bearing sentences. We create three parallel corpora that correspond to three ways to lemmatize & tokenize input (lemmas and original tokens with stopwords removed and BERT-tokenized text). They are stored in the `derived_data/bitext` subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $COLLECT_ROOT/wikipedia_dpr_nq_sample/derived_data/bitext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding document and queries (ANCE, Sentencer BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We already __ship__ data with documents and queries (except for the bitext part) embedded using an [ANCE Wikipedia model](https://github.com/microsoft/ANCE). This is done using the scripts in the `data_convert/biencoder/ance` directory.\n",
    "2. A much more diverse set of embeddings (provided by [Sentence BERT](https://www.sbert.net/)) is available if use the script `data_convert/biencoder/sbert/embed.py`.\n",
    "3. First, one needs to download the models using the script `data_convert/biencoder/ance/download_ance_models.sh`.\n",
    "4. Then, one can embed documents using a command like this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "data_convert/biencoder/ance/embed.py \\\n",
    "    --input $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_raw/psgs_w100.tsv.gz \\\n",
    "    --output $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_data/pass_sample/AnswerFields.bin \\\n",
    "    --field_name dense  \\\n",
    "    --model_dir <model download directory> \\\n",
    "    --data_type dpr_nq \\\n",
    "    --doc_ids collections/wikipedia_dpr_nq_sample/input_raw/nq_selected_psg_ids.npy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ... and queries using a command like this one (note we specify __the binary field name__):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "data_convert/biencoder/ance/embed.py \\\n",
    "    --input collections/wikipedia_dpr_nq_sample/input_raw/psgs_w100.tsv.gz \\\n",
    "    --output collections/wikipedia_dpr_nq_sample/input_data/pass_sample/AnswerFields.bin \\\n",
    "    --field_name dense  \\\n",
    "    --model_dir <model download directory> \\\n",
    "    --data_type dpr_nq \\\n",
    "    --doc_ids collections/wikipedia_dpr_nq_sample/input_raw/nq_selected_psg_ids.npy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for part in train_fusion dev dev_official ; do \\\n",
    "    data_convert/biencoder/ance/embed.py \\\n",
    "        --input $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_data/$part/QuestionFields.jsonl \\\n",
    "        --output $COLLECT_ROOT/wikipedia_dpr_nq_sample/input_data/$part/QuestionFields.bin \\\n",
    "        --field_name dense  \\\n",
    "        --model_dir <model download directory> \\\n",
    "        --data_type dpr_nq \n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
