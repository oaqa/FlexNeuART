{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural model: vanilla BERT ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two things to do before we start:\n",
    "1. Point environment variable `COLLECT_ROOT` to the collection root.\n",
    "2. Change directory to the location of installed scripts/binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%env COLLECT_ROOT=/home/leo/flexneuart_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd /home/leo/flexneuart_scripts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training requires exporting data in the format (with a slight modification) of the \n",
    "CEDR framework ([MacAvaney et al' 2019](https://github.com/Georgetown-IR-Lab/cedr)).\n",
    "\n",
    "The following command\n",
    "generates training data in the CEDR format for the collection `wikipedia_dpr_nq_sample`\n",
    "and the field `text_raw`. The traing data is generated from the split `bitext`, \n",
    "whereas split `dev` is used to generate validation data. During export, one can generate negatives of three types: hard (top-K entries), medium (top-K sample), and easy (sampled from the whole collection). Typically, hard and easy negatives are not particularly useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./export_train/export_cedr.sh \\\n",
    "  wikipedia_dpr_nq_sample \\\n",
    "  text_raw \\\n",
    "  bitext \\\n",
    "  dev \\\n",
    "  -out_subdir cedr_train/text_raw \\\n",
    "  -cand_train_qty 50 \\\n",
    "  -cand_test_qty 50 \\\n",
    "  -thread_qty 4 \\\n",
    "  -hard_neg_qty 0 \\\n",
    "  -sample_easy_neg_qty 0 \\\n",
    "  -sample_med_neg_qty 3 \\\n",
    "  -max_num_query_test 5000 \\\n",
    "  -cand_prov lucene \\\n",
    "  -cand_prov_add_conf exper_desc.best/lucene.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we can use a wrapper convenience script that reads most parameters from a configuration file. \n",
    "\n",
    "Note that the following ``train_model.sh`` scripts assumes that the training data path is **relative** to the ``derived_data`` subdirectory while other paths are **relative** to the collection root. The training script has a number of options (check them out by running with the option ``-h``). Here is how one can run a training script (remember this requires a GPU and pytorch with CUDA support). By default the script validates after each epoch, but this behavior can be changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow-cpu -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./train_nn/train_model.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    cedr_train/text_raw \\\n",
    "     vanilla_bert \\\n",
    "     -seed 0 \\\n",
    "     -add_exper_subdir todays_experiment \\\n",
    "     -json_conf  model_conf/vanilla_bert.json \\\n",
    "     -epoch_qty 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scripts runs, both training and evaluation. The respective statistics is stored in a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"0\": {\r\n",
      "        \"loss\": 0.3602140549432538,\r\n",
      "        \"score\": 0.6625643425430311,\r\n",
      "        \"metric_name\": \"recip_rank\",\r\n",
      "        \"lr\": 0.0002,\r\n",
      "        \"bert_lr\": 2e-05,\r\n",
      "        \"train_time\": 2120.105607032776,\r\n",
      "        \"validation_time\": 507.67602586746216\r\n",
      "    }\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat $COLLECT_ROOT/wikipedia_dpr_nq_sample/derived_data/ir_models/vanilla_bert/todays_experiment/0/train_stat.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is possible to train a neural model in a fusion mode.\n",
    "\n",
    "Here, we optimize for the neural model score fused with the score of a candidate generator. This requires knowing a good weight for the candidate generator score. \n",
    "Here, we assum that the score 1.0 is good enough and export data as shown in the next cell. Please, note the parameter `cand_train_4pos_qty`, which controls the depth of the pool from which we select positive examples. We normally want this pool to be larger than the pool from which we select negative examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./export_train/export_cedr_with_scores.sh \\\n",
    "  wikipedia_dpr_nq_sample \\\n",
    "  text_raw \\\n",
    "  bitext \\\n",
    "  dev \\\n",
    "  -out_subdir cedr_train_with_scores/text_raw \\\n",
    "  -cand_train_qty 50 \\\n",
    "  -cand_test_qty 50 \\\n",
    "  -cand_train_4pos_qty 1000 \\\n",
    "  -thread_qty 4 \\\n",
    "  -hard_neg_qty 0 \\\n",
    "  -sample_easy_neg_qty 0 \\\n",
    "  -sample_med_neg_qty 3 \\\n",
    "  -max_num_query_test 5000 \\\n",
    "  -cand_prov lucene \\\n",
    "  -cand_prov_add_conf exper_desc.best/lucene.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importantly__ to train a model we:\n",
    "1. Use a different configuration file (`model_conf/vanilla_bert_with_scores.json`) that sets candidate provider weights to be non-zero.\n",
    "2. Newly generated training data that exports scores (`cedr_train_with_scores/text_raw`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!./train_nn/train_model.sh \\\n",
    "    wikipedia_dpr_nq_sample \\\n",
    "    cedr_train_with_scores/text_raw \\\n",
    "     vanilla_bert \\\n",
    "     -seed 0 \\\n",
    "     -add_exper_subdir todays_experiment_with_scores \\\n",
    "     -json_conf  model_conf/vanilla_bert_with_scores.json \\\n",
    "     -epoch_qty 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and testing statistics can be found in this JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"0\": {\r\n",
      "        \"loss\": 0.3959440783467357,\r\n",
      "        \"score\": 0.6652820466119467,\r\n",
      "        \"metric_name\": \"recip_rank\",\r\n",
      "        \"lr\": 0.0002,\r\n",
      "        \"bert_lr\": 2e-05,\r\n",
      "        \"train_time\": 2160.42405128479,\r\n",
      "        \"validation_time\": 504.2364339828491\r\n",
      "    }\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat $COLLECT_ROOT/wikipedia_dpr_nq_sample/derived_data/ir_models/vanilla_bert/todays_experiment_with_scores/0/train_stat.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
